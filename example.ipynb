{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import time\n",
    "import tensorflow_datasets as tfds\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from prw import PRW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.seq = keras.Sequential([\n",
    "                          keras.layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "                          keras.layers.LeakyReLU(alpha=0.3),\n",
    "                          keras.layers.Conv2D(64, kernel_size=(3, 3), strides=2, activation=\"relu\"),\n",
    "                          keras.layers.LeakyReLU(alpha=0.3),\n",
    "                          keras.layers.Conv2D(128, kernel_size=(3, 3), strides=2, activation=\"relu\"),\n",
    "                          keras.layers.LeakyReLU(alpha=0.3),\n",
    "                          keras.layers.Conv2D(1, kernel_size=(1, 1), activation=\"relu\"),\n",
    "                          keras.layers.Flatten(),\n",
    "                          keras.layers.Dense(10, activation=\"softmax\")\n",
    "                         ])\n",
    "        \n",
    "    @tf.function\n",
    "    def call(self, x):\n",
    "        return self.seq(x)\n",
    "        \n",
    "    @tf.function\n",
    "    def train_step(self, data):\n",
    "        if len(data) == 3:\n",
    "            return self.train_on_embedding(data)\n",
    "        return self.train_on_normal(data)\n",
    "    \n",
    "    @tf.function\n",
    "    def train_on_normal(self, data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True)            \n",
    "            loss = self.loss(y, y_pred)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        return {\"loss\": loss}\n",
    "        \n",
    "    @tf.function\n",
    "    def train_on_embedding(self, data):\n",
    "        train, null_e, true_e = data\n",
    "        x_train, y_train = train\n",
    "        x_null_e, y_null_e = null_e\n",
    "        x_true_e, y_true_e = true_e\n",
    "        with tf.GradientTape(persistent=True) as train_tape: \n",
    "            train_pred = self(x_train, training=True)            \n",
    "            train_loss = self.loss(y_train, train_pred)\n",
    "            null_e_pred = self(x_null_e, training=True)\n",
    "            null_e_loss = 0.25 * self.loss(y_null_e, null_e_pred)\n",
    "            true_e_pred = self(x_true_e, training=True)\n",
    "            true_e_loss = 0.25 * self.loss(y_true_e, true_e_pred)\n",
    "\n",
    "        train_grads = train_tape.gradient(train_loss, self.trainable_variables)\n",
    "        null_e_grads = train_tape.gradient(null_e_loss, self.trainable_variables)\n",
    "        true_e_grads = train_tape.gradient(true_e_loss, self.trainable_variables)\n",
    "        \n",
    "        gradients = []\n",
    "        for train_grad, null_e_grad, true_e_grad in zip(train_grads, null_e_grads, true_e_grads):\n",
    "            gradients.append(train_grad + null_e_grad + true_e_grad)\n",
    "        \n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        \n",
    "        return {\n",
    "            \"train_loss\": train_loss, \n",
    "            \"null_embedding_loss\": null_e_loss, \n",
    "            \"true_embedding_loss\": true_e_loss,\n",
    "            \"total_loss\": train_loss + null_e_loss + true_e_loss\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uniqid = \"edfdabd4-7c42-4559-9335-36f282c2899f\"\n",
    "v = uniqid + \"_\" + str(int(time.time()))\n",
    "\n",
    "def cast_to_float(example, label):\n",
    "    return tf.cast(example, tf.float64) / 255.0, label\n",
    "\n",
    "prw = PRW()\n",
    "sig = prw.create_signature(v)\n",
    "y_w = prw.transform(sig, 28, 28, 10, 6, 1)\n",
    "\n",
    "\n",
    "ds_train = tfds.load(\"mnist\", split=\"train\", as_supervised=True)\n",
    "ds_test = tfds.load(\"mnist\", split=\"test\", as_supervised=True)\n",
    "\n",
    "ds_train = ds_train.map(cast_to_float)\n",
    "ds_test = ds_test.map(cast_to_float)\n",
    "\n",
    "ds_train_null = ds_train.map(prw.apply_null_embedding)\n",
    "ds_train_embed = ds_train.map(prw.apply_true_embedding)\n",
    "\n",
    "# plot an example null embeded image\n",
    "for x_batch, y_batch in ds_train_null.take(1):\n",
    "    example = x_batch.numpy().reshape((28, 28)).clip(0, 1)\n",
    "    plt.imshow(example)\n",
    "    plt.show()\n",
    "    \n",
    "# plot an example true embedded image\n",
    "for x_batch, y_batch in ds_train_embed.take(1):\n",
    "    example = x_batch.numpy().reshape((28, 28)).clip(0, 1)\n",
    "    plt.imshow(example)\n",
    "    plt.show()\n",
    "\n",
    "ds_full_train = tf.data.Dataset.zip((ds_train, ds_train_null, ds_train_embed))\n",
    "\n",
    "optim = tf.keras.optimizers.SGD(learning_rate=0.001, momentum=0.9)\n",
    "\n",
    "ds_full_train = ds_full_train.shuffle(2048).batch(128).prefetch(-1)\n",
    "ds_test = ds_test.batch(128)\n",
    "model = MyModel()\n",
    "model.build(input_shape=(None, 28, 28, 1))\n",
    "model.compile(optimizer=optim, loss=keras.losses.SparseCategoricalCrossentropy(), metrics=[\"sparse_categorical_accuracy\"])\n",
    "model.fit(epochs=10, x=ds_full_train, validation_data=ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "succ = prw.verify(model, 28, 28, 10, 6, sig, v, ds_train, 0.8, True)\n",
    "\n",
    "if succ:\n",
    "    print(\"Verification was successful, it's our model.\")\n",
    "else:\n",
    "    print(\"Verification was not successful, maybe it's not our model.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds_train = tfds.load(\"mnist\", split=\"train\", as_supervised=True)\n",
    "ds_test = tfds.load(\"mnist\", split=\"test\", as_supervised=True)\n",
    "ds_train = ds_train.map(cast_to_float)\n",
    "ds_test = ds_test.map(cast_to_float)\n",
    "\n",
    "ds_train = ds_train.shuffle(2048).batch(128).prefetch(-1)\n",
    "ds_test = ds_test.batch(128)\n",
    "\n",
    "zero_epoch_acc = model.evaluate(ds_test, return_dict=True)['sparse_categorical_accuracy']\n",
    "\n",
    "model.fit(epochs=10, x=ds_test, validation_data=ds_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
